{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e085fa51",
   "metadata": {},
   "source": [
    "#### Firstly import everything.\n",
    "Using:\n",
    "```\n",
    "LabelEncoder for labeling the target ner values and encoding it as numbers.\n",
    "Hugging Face models.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bff8a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from seqeval.metrics import classification_report\n",
    "import json\n",
    "import joblib\n",
    "# !pip install transformers torch\n",
    "# !pip install seqeval==0.0.10\n",
    "# !pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "47916f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_df(path):\n",
    "    rows = []\n",
    "    sentence_id = 0\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as file_text:\n",
    "        for line in file_text:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                sentence_id += 1\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"-DOCSTART-\"):\n",
    "                continue\n",
    "\n",
    "            token, pos, chunk, ner = line.split()\n",
    "            rows.append({\"sentence_id\": sentence_id, \"token\": token, \"pos\": pos, \"chunk\": chunk, \"ner\": ner})\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "926c3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = _to_df(os.path.join(\"_resources\", \"train.txt\"))\n",
    "test_df = _to_df(os.path.join(\"_resources\", \"test.txt\"))\n",
    "valid_df = _to_df(os.path.join(\"_resources\", \"valid.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "dc872d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df: pd.DataFrame):\n",
    "\treturn df.groupby(\"sentence_id\")[['token', 'pos', 'chunk', 'ner']]\\\n",
    "\t\t.apply(lambda x: tuple(map(tuple, x.values)))\\\n",
    "\t\t.duplicated().sum()\n",
    "\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "bf026d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sentence_duplicates(df: pd.DataFrame):\n",
    "    sentence_repr = df.groupby(\"sentence_id\")[['token', 'pos', 'chunk', 'ner']]\\\n",
    "                      .apply(lambda x: tuple(map(tuple, x.values)))\n",
    "\n",
    "    unique_sentence_ids = sentence_repr[~sentence_repr.duplicated()].index\n",
    "\n",
    "    return df[df[\"sentence_id\"].isin(unique_sentence_ids)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "2826c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate (Sentences) in Training: 1348\n",
      "Duplicate (Sentences) in Validation: 179\n",
      "Duplicate (Sentences) in Testing: 266\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Duplicate (Sentences) in Training: {check_duplicates(train_df)}\n",
    "Duplicate (Sentences) in Validation: {check_duplicates(valid_df)}\n",
    "Duplicate (Sentences) in Testing: {check_duplicates(test_df)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a4105c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop_sentence_duplicates(train_df)\n",
    "valid_df = drop_sentence_duplicates(valid_df)\n",
    "test_df = drop_sentence_duplicates(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "a061cf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence_id', 'token', 'pos', 'chunk', 'ner']"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "fb474a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df.loc[:, 'is_capitalized'] = df['token'].str[0].str.isupper().astype(int)\n",
    "    df.loc[:, 'is_all_caps'] = df['token'].str.isupper().astype(int)\n",
    "    df.loc[:, 'is_all_lower'] = df['token'].str.islower().astype(int)\n",
    "    df.loc[:, 'token_len'] = df['token'].str.len().astype(int)\n",
    "    df.loc[:, 'contains_digit'] = df['token'].str.contains(r'\\d', regex=True).astype(int)\n",
    "    df.loc[:, 'contains_dash'] = df['token'].str.contains('-').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3cdaea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features(train_df)\n",
    "add_features(valid_df)\n",
    "add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df_list):\n",
    "    all_tags = pd.concat([df[\"ner\"] for df in df_list]).unique()\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_tags)\n",
    "\n",
    "    for df in df_list:\n",
    "        df[\"ner_id\"] = label_encoder.transform(df[\"ner\"])\n",
    "\n",
    "    return label_encoder\n",
    "\n",
    "def add_sentence_position(df):\n",
    "    return df.groupby(\"sentence_id\").apply(\n",
    "        lambda s: s.assign(pos_in_sentence=[(i + 1) / len(s) for i in range(len(s))])\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "def encode_for_transformers(df, tokenizer, max_len=50):\n",
    "    grouped = df.groupby(\"sentence_id\").apply(\n",
    "        lambda s: {\n",
    "            \"tokens\": list(s[\"token\"]),\n",
    "            \"labels\": list(s[\"ner_id\"]),\n",
    "            \"pos_in_sentence\": list(s[\"pos_in_sentence\"]),\n",
    "            \"is_capitalized\": list(s[\"is_capitalized\"]),\n",
    "            \"is_all_caps\": list(s[\"is_all_caps\"]),\n",
    "            \"is_all_lower\": list(s[\"is_all_lower\"]),\n",
    "            \"token_len\": list(s[\"token_len\"]),\n",
    "            \"contains_digit\": list(s[\"contains_digit\"]),\n",
    "            \"contains_dash\": list(s[\"contains_dash\"])\n",
    "        }\n",
    "    ).tolist()\n",
    "\n",
    "    tokens_list = [[str(tok) for tok in g[\"tokens\"]] for g in grouped]\n",
    "    encodings = tokenizer(tokens_list, is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n",
    "\n",
    "    aligned_labels, aligned_positions = [], []\n",
    "    extras = [(\"is_capitalized\", -1), (\"is_all_caps\", -1), (\"is_all_lower\", -1), (\"token_len\", 0), (\"contains_digit\", -1), (\"contains_dash\", -1)]\n",
    "    aligned_extra = {c[0]: ([], c[1]) for c in extras}\n",
    "\n",
    "    for i, g in enumerate(grouped):\n",
    "        word_ids = encodings.word_ids(batch_index=i)\n",
    "        \n",
    "        labels = [-100 if w_id is None else g[\"labels\"][w_id] for w_id in word_ids]\n",
    "        positions = [0.0 if w_id is None else g[\"pos_in_sentence\"][w_id] for w_id in word_ids]\n",
    "        \n",
    "        aligned_labels.append(labels)\n",
    "        aligned_positions.append(positions)\n",
    "\n",
    "        for feat, pad in aligned_extra.items():\n",
    "            aligned_extra[feat][0].append([pad[1] if w_id is None else g[feat][w_id] for w_id in word_ids])\n",
    "    \n",
    "        if i < 3:\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"[DEBUG] Sentence {i + 1}\")\n",
    "            print(f\"Tokens: {g['tokens']}\")\n",
    "            print(f\"Original Labels: {g['labels']}\")\n",
    "            print(f\"Positions: {g['pos_in_sentence']}\")\n",
    "\n",
    "            tokens_subwords = tokenizer.convert_ids_to_tokens(encodings[\"input_ids\"][i])\n",
    "            print(f\"Tokenized (with subwords): {tokens_subwords}\")\n",
    "            print(f\"Word IDs: {word_ids}\")\n",
    "            print(f\"Aligned Labels: {labels}\")\n",
    "            print(f\"Aligned Positions: {positions}\")\n",
    "            for feat in aligned_extra:\n",
    "                print(f\"Aligned {feat}: {aligned_extra[feat][0][-1]}\")\n",
    "\n",
    "    encodings[\"labels\"] = torch.tensor(aligned_labels, dtype=torch.long)\n",
    "    encodings[\"pos_in_sentence\"] = torch.tensor(aligned_positions, dtype=torch.float32)\n",
    "\n",
    "    for feat in aligned_extra:\n",
    "        encodings[feat] = torch.tensor(aligned_extra[feat][0], dtype=torch.float32)\n",
    "\n",
    "    return encodings\n",
    "\n",
    "def save_preprocessed_data(train_df, test_df, valid_df):\n",
    "    combined = pd.concat([\n",
    "        train_df.assign(split=\"train\"),\n",
    "        test_df.assign(split=\"test\"),\n",
    "        valid_df.assign(split=\"valid\")\n",
    "    ], ignore_index=True)\n",
    "    path = \"./_resources/preprocessed.csv\"\n",
    "    combined.to_csv(path, index=False)\n",
    "    print(f\"Preprocessed data saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "dfd89940",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./_resources/models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c811e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG',\n",
       "       'I-PER', 'O'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = encode_labels([train_df, test_df, valid_df])\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "1296546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_sentence_position(train_df)\n",
    "test_df = add_sentence_position(test_df)\n",
    "valid_df = add_sentence_position(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "437bc59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessed data saved to ./_resources/preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "save_preprocessed_data(train_df, test_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57e0f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "\tdf: pd.DataFrame = pd.read_csv(\"./_resources/preprocessed.csv\").astype({\n",
    "\t\t\"sentence_id\": \"int64\",\n",
    "\t\t\"token\": \"string\",\n",
    "\t\t\"pos\": \"string\",\n",
    "\t\t\"chunk\": \"string\",\n",
    "\t\t\"ner\": \"string\",\n",
    "\t\t\"is_capitalized\": \"int64\",\n",
    "\t\t\"is_all_caps\": \"int64\",\n",
    "\t\t\"is_all_lower\": \"int64\",\n",
    "\t\t\"token_len\": \"int64\",\n",
    "\t\t\"contains_digit\": \"int64\",\n",
    "\t\t\"contains_dash\": \"int64\",\n",
    "\t\t\"ner_id\": \"int64\",\n",
    "\t\t\"pos_in_sentence\": \"float64\",\n",
    "\t\t\"split\": \"string\"\n",
    "\t})\n",
    "\t_train_df = df[df['split'] == 'train']\n",
    "\t_valid_df = df[df['split'] == 'valid']\n",
    "\t_test_df = df[df['split'] == 'test']\n",
    "\n",
    "\treturn _train_df, _valid_df, _test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51a7905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bff917c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eef45a65f63443eace97c99c8447252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Applications\\python-projects\\eleevo-internship\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\2024\\.cache\\huggingface\\hub\\models--google--electra-small-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639667c1088c4e308dc2d0bc381c8afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639436cf0df3432e9d401fdf8f251c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af94eb711694978a496cd631487edfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "# (\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f329cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[DEBUG] Sentence 1\n",
      "Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Original Labels: [2, 8, 1, 8, 8, 8, 1, 8, 8]\n",
      "Positions: [0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 2, 8, 1, 8, 8, 8, 1, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 0, 1, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 1, 0, 1, 1, 1, 0, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 2, 7, 6, 4, 2, 7, 7, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 2\n",
      "Tokens: ['Peter', 'Blackburn']\n",
      "Original Labels: [3, 7]\n",
      "Positions: [0.5, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'peter', 'blackburn', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 3, 7, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 5, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 3\n",
      "Tokens: ['BRUSSELS', '1996-08-22']\n",
      "Original Labels: [0, 8]\n",
      "Positions: [0.5, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'brussels', '1996', '-', '08', '-', '22', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 0, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 8, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 1\n",
      "Tokens: ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "Original Labels: [8, 8, 0, 8, 8, 8, 8, 3, 8, 8, 8, 8]\n",
      "Positions: [0.0833333333333333, 0.1666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 8, 8, 0, 8, 8, 8, 8, 3, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.0833333333333333, 0.1666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 6, 1, 5, 3, 5, 3, 1, 5, 2, 8, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 2\n",
      "Tokens: ['Nadim', 'Ladki']\n",
      "Original Labels: [3, 7]\n",
      "Positions: [0.5, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'nad', '##im', 'lad', '##ki', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 0, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 3, 3, 7, 7, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 3\n",
      "Tokens: ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06']\n",
      "Original Labels: [0, 8, 0, 4, 4, 8]\n",
      "Positions: [0.1666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333334, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'al', '-', 'ain', ',', 'united', 'arab', 'emirates', '1996', '-', '12', '-', '06', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 0, 0, 0, 8, 0, 4, 4, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.1666666666666666, 0.1666666666666666, 0.1666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 6, 6, 6, 1, 6, 4, 8, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 1\n",
      "Tokens: ['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.']\n",
      "Original Labels: [8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "Positions: [0.0909090909090909, 0.1818181818181818, 0.2727272727272727, 0.3636363636363636, 0.4545454545454545, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.9090909090909092, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.0909090909090909, 0.1818181818181818, 0.2727272727272727, 0.3636363636363636, 0.4545454545454545, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.9090909090909092, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 7, 1, 14, 4, 4, 2, 3, 5, 7, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 2\n",
      "Tokens: ['LONDON', '1996-08-30']\n",
      "Original Labels: [0, 8]\n",
      "Positions: [0.5, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'london', '1996', '-', '08', '-', '30', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 0, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 6, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "==================================================\n",
      "[DEBUG] Sentence 3\n",
      "Tokens: ['West', 'Indian', 'all-rounder', 'Phil', 'Simmons', 'took', 'four', 'for', '38', 'on', 'Friday', 'as', 'Leicestershire', 'beat', 'Somerset', 'by', 'an', 'innings', 'and', '39', 'runs', 'in', 'two', 'days', 'to', 'take', 'over', 'at', 'the', 'head', 'of', 'the', 'county', 'championship', '.']\n",
      "Original Labels: [1, 5, 8, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "Positions: [0.0285714285714285, 0.0571428571428571, 0.0857142857142857, 0.1142857142857142, 0.1428571428571428, 0.1714285714285714, 0.2, 0.2285714285714285, 0.2571428571428571, 0.2857142857142857, 0.3142857142857143, 0.3428571428571428, 0.3714285714285714, 0.4, 0.4285714285714285, 0.4571428571428571, 0.4857142857142857, 0.5142857142857142, 0.5428571428571428, 0.5714285714285714, 0.6, 0.6285714285714286, 0.6571428571428571, 0.6857142857142857, 0.7142857142857143, 0.7428571428571429, 0.7714285714285715, 0.8, 0.8285714285714286, 0.8571428571428571, 0.8857142857142857, 0.9142857142857144, 0.9428571428571428, 0.9714285714285714, 1.0]\n",
      "Tokenized (with subwords): ['[CLS]', 'west', 'indian', 'all', '-', 'round', '##er', 'phil', 'simmons', 'took', 'four', 'for', '38', 'on', 'friday', 'as', 'leicestershire', 'beat', 'somerset', 'by', 'an', 'innings', 'and', '39', 'runs', 'in', 'two', 'days', 'to', 'take', 'over', 'at', 'the', 'head', 'of', 'the', 'county', 'championship', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Word IDs: [None, 0, 1, 2, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "Aligned Labels: [-100, 1, 5, 8, 8, 8, 8, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Aligned Positions: [0.0, 0.0285714285714285, 0.0571428571428571, 0.0857142857142857, 0.0857142857142857, 0.0857142857142857, 0.0857142857142857, 0.1142857142857142, 0.1428571428571428, 0.1714285714285714, 0.2, 0.2285714285714285, 0.2571428571428571, 0.2857142857142857, 0.3142857142857143, 0.3428571428571428, 0.3714285714285714, 0.4, 0.4285714285714285, 0.4571428571428571, 0.4857142857142857, 0.5142857142857142, 0.5428571428571428, 0.5714285714285714, 0.6, 0.6285714285714286, 0.6571428571428571, 0.6857142857142857, 0.7142857142857143, 0.7428571428571429, 0.7714285714285715, 0.8, 0.8285714285714286, 0.8571428571428571, 0.8857142857142857, 0.9142857142857144, 0.9428571428571428, 0.9714285714285714, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Aligned is_capitalized: [-1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_caps: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned is_all_lower: [-1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned token_len: [0, 4, 6, 11, 11, 11, 11, 4, 7, 4, 4, 3, 2, 2, 6, 2, 14, 4, 8, 2, 2, 7, 3, 2, 4, 2, 3, 4, 2, 4, 4, 2, 3, 4, 2, 3, 6, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aligned contains_digit: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Aligned contains_dash: [-1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "train_enc = encode_for_transformers(train_df, tokenizer, 60)\n",
    "test_enc = encode_for_transformers(test_df, tokenizer, 60)\n",
    "valid_enc = encode_for_transformers(valid_df, tokenizer, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03414025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NERWithFeatures(nn.Module):\n",
    "    def __init__(self, num_labels, transformer_model=\"google-bert/bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        # self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_model)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "        # for name, param in self.transformer.named_parameters():\n",
    "        #     if \"layer\" in name and int(name.split(\".\")[2]) < 8:\n",
    "        #         param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = transformer_outputs.last_hidden_state\n",
    "        token_embeddings = self.dropout(token_embeddings)\n",
    "        logits = self.classifier(token_embeddings)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(NERDataset(train_enc), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(NERDataset(valid_enc), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a0b96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_labels_len = len(train_df['ner_id'].unique())\n",
    "model = NERWithFeatures(_labels_len, transformer_model=\"google/electra-small-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f17d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 199/199 [10:31<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.3451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 199/199 [10:32<00:00,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "optimizer = AdamW(model.parameters())\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b1a0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, labels)\n",
    "        active_loss = labels.view(-1) != -100\n",
    "        active_logits = logits.view(-1, _labels_len)[active_loss]\n",
    "        active_labels = labels.view(-1)[active_loss]\n",
    "        loss = nn.functional.cross_entropy(active_logits, active_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, label_encoder, tokenizer, save_predictions=False):\n",
    "    model.eval()\n",
    "    token_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_ids[i], skip_special_tokens=False)\n",
    "                active = labels[i] != -100\n",
    "                active_tokens = [t for t, a in zip(tokens, active) if a]\n",
    "                \n",
    "                active_labels = labels[i][active].cpu().numpy()\n",
    "                active_preds = preds[i][active]\n",
    "                decoded_labels = label_encoder.inverse_transform(active_labels)\n",
    "                decoded_preds = label_encoder.inverse_transform(active_preds)\n",
    "                \n",
    "                for token, actual, pred in zip(active_tokens, decoded_labels, decoded_preds):\n",
    "                    token_predictions.append((token, actual, pred))\n",
    "    \n",
    "    actuals = [t[1] for t in token_predictions]\n",
    "    preds = [t[2] for t in token_predictions]\n",
    "\n",
    "    validation_report = classification_report(actuals, preds)\n",
    "\n",
    "    print(\"\\nEvaluating on a data set:\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(validation_report)\n",
    "    print(\"\\nSample Token Predictions (first 20):\")\n",
    "    for t in token_predictions[:20]:\n",
    "        print(t)\n",
    "\n",
    "    if save_predictions:\n",
    "        os.makedirs(\"./_resources\", exist_ok=True)\n",
    "        with open(\"./_resources/ner_predictions.json\", \"w\") as f:\n",
    "            json.dump(token_predictions, f, indent=2)\n",
    "\n",
    "    return validation_report, token_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68e512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 199/199 [10:21<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 199/199 [10:24<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    evaluate(model, valid_loader, label_encoder, tokenizer)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "83deb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, label_encoder, save_dir=\"./_resources/models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.joblib\"))\n",
    "    print(f\"Model, tokenizer, and label encoder saved to {save_dir}\")\n",
    "\n",
    "def load_model(_train_df, save_dir=\"./_resources/models\", transformer_model=\"google/electra-small-discriminator\"):\n",
    "    _labels_len = len(_train_df['ner_id'].unique())\n",
    "    model = NERWithFeatures(_labels_len, transformer_model=transformer_model)\n",
    "    model.load_state_dict(torch.load(os.path.join(save_dir, \"model.pth\")))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "    label_encoder = joblib.load(os.path.join(save_dir, \"label_encoder.joblib\"))\n",
    "    print(f\"Model, tokenizer, and label encoder loaded from {save_dir}\")\n",
    "    return model, tokenizer, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f437ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and label encoder saved to ./_resources/models\n",
      "Model, tokenizer, and label encoder loaded from ./_resources/models\n"
     ]
    }
   ],
   "source": [
    "save_model(model, tokenizer, label_encoder)\n",
    "model, tokenizer, label_encoder = load_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a0ca4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 50/50 [00:39<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on a data set:\n",
      "\n",
      "Classification report:\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.85      0.89      0.87      1940\n",
      "      PER       0.87      0.92      0.89      2644\n",
      "     MISC       0.71      0.66      0.68       991\n",
      "      ORG       0.79      0.78      0.78      2498\n",
      "\n",
      "micro avg       0.82      0.83      0.83      8073\n",
      "macro avg       0.82      0.83      0.83      8073\n",
      "\n",
      "\n",
      "Sample Token Predictions (first 20):\n",
      "('soccer', 'O', 'O')\n",
      "('-', 'O', 'O')\n",
      "('japan', 'B-LOC', 'B-LOC')\n",
      "('get', 'O', 'O')\n",
      "('lucky', 'O', 'O')\n",
      "('win', 'O', 'O')\n",
      "(',', 'O', 'O')\n",
      "('china', 'B-PER', 'B-LOC')\n",
      "('in', 'O', 'O')\n",
      "('surprise', 'O', 'O')\n",
      "('defeat', 'O', 'O')\n",
      "('.', 'O', 'O')\n",
      "('nad', 'B-PER', 'B-PER')\n",
      "('##im', 'B-PER', 'B-PER')\n",
      "('lad', 'I-PER', 'I-PER')\n",
      "('##ki', 'I-PER', 'I-PER')\n",
      "('al', 'B-LOC', 'B-LOC')\n",
      "('-', 'B-LOC', 'B-LOC')\n",
      "('ain', 'B-LOC', 'B-LOC')\n",
      "(',', 'O', 'O')\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(NERDataset(test_enc), batch_size=batch_size)\n",
    "validation_report, predictions = evaluate(model, test_loader, label_encoder, tokenizer, save_predictions=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
